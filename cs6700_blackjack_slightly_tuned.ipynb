{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blackjack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Action Player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import keras.layers as layers\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMemoryUnit():\n",
    "    def __init__(self, state, action, reward, done, next_state):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.done = done\n",
    "        self.next_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_nan(y_true, y_pred):\n",
    "    index = ~K.tf.is_nan(y_true)\n",
    "    y_true = K.tf.boolean_mask(y_true, index)\n",
    "    y_pred = K.tf.boolean_mask(y_pred, index)\n",
    "    return K.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100000 [00:00<?, ?it/s]\u001b[A/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\n",
      "Exception in thread Thread-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/anaconda/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      " 12%|█▏        | 11765/100000 [40:50<5:06:20,  4.80it/s]"
     ]
    }
   ],
   "source": [
    "memory_size = 10000\n",
    "episodes = 100000\n",
    "epsilon = 0.1\n",
    "gamma = 0.2\n",
    "minibatch_size = 32\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=3, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss=mse_nan, optimizer=RMSprop())\n",
    "\n",
    "memory = deque(maxlen=memory_size)\n",
    "env = gym.make('Blackjack-v0')\n",
    "test_env = gym.make('Blackjack-v0')\n",
    "\n",
    "action_space = [0, 1]\n",
    "\n",
    "steps = 0\n",
    "reward_plot = []\n",
    "loss_plot = []\n",
    "rewards = []\n",
    "for episode in tqdm(range(episodes)):\n",
    "    done = False\n",
    "    state = np.array(env.reset()).reshape(1,3)\n",
    "    total_reward = 0\n",
    "    while(not done):\n",
    "        \n",
    "        #random choice\n",
    "        if(random.random() < epsilon):\n",
    "            action = np.random.choice(action_space)\n",
    "        \n",
    "        #action with greatest expected value\n",
    "        else:\n",
    "            action = np.argmax(model.predict(state))\n",
    "            \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        memory.append(QMemoryUnit(state, action, reward, done, next_state))\n",
    "        state = np.array(next_state).reshape(1,3)\n",
    "        \n",
    "        minibatch = np.zeros((minibatch_size, 3))\n",
    "        labels = np.zeros((minibatch_size, 2))\n",
    "        labels.fill(np.nan)\n",
    "        #generate minibatch and labels\n",
    "        for i in range(minibatch_size):\n",
    "            index = np.random.randint(0, len(memory))\n",
    "            sample = memory[index]\n",
    "            \n",
    "            reward = sample.reward\n",
    "            if(not sample.done):\n",
    "                reward += gamma * np.max(model.predict(np.array(sample.next_state).reshape(1,3)))\n",
    "            \n",
    "            minibatch[i] = sample.state\n",
    "            labels[i, sample.action] = reward\n",
    "            \n",
    "        history = model.fit(minibatch, labels, verbose=0)\n",
    "        loss_plot.append(history.history['loss'])\n",
    "        if(steps%50 == 0):\n",
    "            total_reward = 0\n",
    "            for i in range(1000):\n",
    "                test_env.seed(i)\n",
    "                state = test_env.reset()\n",
    "                done = False\n",
    "                while(not done):\n",
    "                    action = np.argmax(model.predict(np.array(state).reshape(1,3)))\n",
    "                    state, reward, done, _ = test_env.step(action)\n",
    "                total_reward += reward\n",
    "            reward_plot.append(total_reward)\n",
    "            \n",
    "            \n",
    "        steps+=1\n",
    "        \n",
    "    rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "for i in range(1000):\n",
    "    env.seed(i)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while(not done):\n",
    "        action = np.argmax(model.predict(np.array(state).reshape(1,3)))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_plot)\n",
    "plt.xlabel('Training Step/50')\n",
    "plt.ylabel('Total Reward over 1000 Games')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
